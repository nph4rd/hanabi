inference_gpu_ids = [0,1,2,3,4,5]
trainer_gpu_ids = [6,7]

max_steps = 100

[wandb]
project = "hanabi"
name = "hanabi"

[ckpt] # Checkpoint at the end of training

[model]
name = "nph4rd/Qwen3-1.7B-Hanabi-SFT"

[orchestrator]
seq_len = 8192
batch_size = 256
rollouts_per_example = 16
trajectory_strategy = "branching"

[[orchestrator.env]]
id = "nph4rd/hanabi"
name = "hanabi"
args = {"num_players" = 2, max_turns = 20}

[orchestrator.sampling]
max_tokens = 512

[trainer] # Default trainer config

[inference]
gpu_memory_utilization = 0.8

[inference.parallel]
dp = 3
tp = 2