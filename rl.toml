
inference_gpu_ids = [0,1]
trainer_gpu_ids = [2,3,4,5,6,7]

max_steps = 100
seq_len = 16384

[ckpt] # Checkpoint at the end of training

[model]
name = "nph4rd/Qwen3-4B-Instruct-2507-Hanabi-SFT"

[wandb]
project = "hanabi-rl"
name = "hanabi-rl"

[trainer]

[orchestrator]
batch_size = 64
rollouts_per_example = 16
trajectory_strategy = "branching"

[orchestrator.sampling]
max_tokens = 128

[[orchestrator.env]]
id = "nph4rd/hanabi"
name = "hanabi"

[inference.model]
enable_auto_tool_choice = true
tool_call_parser = "hermes"